{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HomeWork #1 â€“ Genre Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.svm #containts the class that implements the machine to analyze music\n",
    "import IPython.display as ipd\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we skip the preliminary elaboration of the signal \n",
    "def compute_mfcc(audio, fs, n_mfcc):\n",
    "    # Compute the spectrogram of the audio signal. We take the magnitude of the signal, windowing it\n",
    "    X = np.abs(librosa.stft(\n",
    "        audio,\n",
    "        window='hamming',\n",
    "        n_fft=1024, #number of points in which fft will be done in window.\n",
    "        hop_length=512,)\n",
    "        )\n",
    "    \n",
    "    # Find the weights of the mel filters (in librosa) we must specify: the original f of the signal, what are the points used for \n",
    "    #STFF (Short Transfer Fourier Function), how many rectangular filters we want and Min and Max Frequency of the filter. \n",
    "    mel = librosa.filters.mel(\n",
    "        sr=fs,\n",
    "        n_fft=1024,\n",
    "        n_mels=40,\n",
    "        fmin=133.33, #in hz, we decide it. those values are pretty standard\n",
    "        fmax=6853.8,\n",
    "    )\n",
    "    \n",
    "    # Apply the filters to spectrogram\n",
    "    melspectrogram = np.dot(mel, X)\n",
    "    # Take the logarithm\n",
    "    log_melspectrogram = np.log10(melspectrogram + 1e-16) #we added 1e-16 just to avoid log(0) but is an inignificant value.\n",
    "    \n",
    "    # Apply the DCT to log melspectrogram to obtain the coefficients\n",
    "    mfcc = sp.fftpack.dct(log_melspectrogram, axis=0, norm='ortho')[1:n_mfcc+1]\n",
    "    return mfcc #n_mfcc x no. of windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['rock', 'pop', 'reggae','disco'] #now I wanna train my algorithm.\n",
    "\n",
    "# we compute the number of factors for each feature\n",
    "n_mfcc = 35\n",
    "n_chroma = 12\n",
    "n_spec_roll = 1\n",
    "n_spec_centr = 1\n",
    "n_feat_weights = n_mfcc + n_chroma + n_spec_roll + n_spec_centr\n",
    "dict_train_features = {'rock': [], 'pop': [], 'reggae': [], 'disco': []}\n",
    "mfcc_plot = {'rock': [], 'pop': [], 'reggae': [], 'disco': []} # for plotting the MFCC figures\n",
    "\n",
    "\n",
    "for c in classes:\n",
    "    # print(\"Sto analizzando \" + '{}'.format(c))\n",
    "    train_root = '/Users/matteosartori/opt/miniconda3/CMLS/PreProcessedRed/{}/train'.format(c)\n",
    "    class_train_files = [f for f in os.listdir(train_root) if f.endswith('.mp3')]\n",
    "    n_train_samples = len(class_train_files)\n",
    "    \n",
    "    train_mfcc_plot = np.zeros((n_train_samples, n_mfcc))\n",
    "    train_features = np.zeros((n_train_samples, n_feat_weights))\n",
    "    \n",
    "    # we compute for each track: mfcc_mean, chroma_mean, spec_roll_mean and spec_centr_mean \n",
    "    for index, f in enumerate(class_train_files):\n",
    "        audio, fs = librosa.load(os.path.join(train_root, f), sr=None)\n",
    "        mfcc = compute_mfcc(audio, fs, n_mfcc)\n",
    "        mfcc_mean = np.mean(mfcc, axis = 1)\n",
    "        chroma = librosa.feature.chroma_stft(audio, fs)\n",
    "        chroma_mean = np.mean(chroma, axis = 1)\n",
    "        spec_roll = librosa.feature.spectral_rolloff(audio, fs)\n",
    "        \n",
    "        \n",
    "        ### Spectral Rolloff Plot\n",
    "        \n",
    "        # X = np.abs(librosa.stft(\n",
    "        # audio,\n",
    "        # window='hamming',\n",
    "        # n_fft=1024, #number of points in which fft will be done in window.\n",
    "        # hop_length=512,)\n",
    "        # )\n",
    "        \n",
    "        # rolloff = 0.99*spec_roll\n",
    "        # rolloff_min = 0.01*spec_roll\n",
    "\n",
    "        # fig, ax = plt.subplots()\n",
    "        # librosa.display.specshow(librosa.amplitude_to_db(X, ref=np.max), y_axis='log', x_axis='time', ax=ax)\n",
    "        # ax.plot(librosa.times_like(rolloff), rolloff[0], label='Roll-off frequency (0.99)')\n",
    "        # ax.plot(librosa.times_like(rolloff), rolloff_min[0], color='w', label='Roll-off frequency (0.01)')\n",
    "        # ax.legend(loc='lower right')\n",
    "        # ax.set(title='log Power spectrogram')\n",
    "        \n",
    "        ## valid code but plots for library updates do not come out that we cannot modify (basey-> base; linscaley _> linscale)\n",
    "        ## to not have a thousand plots you can use the spec_roll_mean defined below instead of the spec_roll, outside this for loop\n",
    "        \n",
    "        \n",
    "        \n",
    "        spec_roll_mean = np.mean(spec_roll, axis = 1)\n",
    "        spec_centr = librosa.feature.spectral_centroid(audio, fs)\n",
    "        spec_centr_mean = np.mean(spec_centr, axis = 1)\n",
    "        train_mfcc_plot[index, :] = np.mean(mfcc, axis=1)\n",
    "        train_features[index, :] = np.concatenate((mfcc_mean, chroma_mean, spec_roll_mean, spec_centr_mean), axis = 0)\n",
    "    mfcc_plot[c] = train_mfcc_plot\n",
    "    dict_train_features[c] = train_features\n",
    "   # print(dict_train_features[c].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the syntax for test files is pretty the same. \n",
    "dict_test_features = {'rock': [], 'pop': [], 'reggae': [], 'disco': []}\n",
    "\n",
    "for c in classes:\n",
    "    test_root = '/Users/matteosartori/opt/miniconda3/CMLS/PreProcessedRed/{}/test'.format(c)\n",
    "    class_test_files = [f for f in os.listdir(test_root) if f.endswith('.mp3')]\n",
    "    n_test_samples = len(class_test_files)\n",
    "    \n",
    "    test_features = np.zeros((n_test_samples, n_feat_weights))\n",
    "    for index, f in enumerate(class_test_files):\n",
    "        audio, fs = librosa.load(os.path.join(test_root, f), sr=None)\n",
    "        mfcc = compute_mfcc(audio, fs, n_mfcc)\n",
    "        mfcc_mean = np.mean(mfcc, axis = 1)\n",
    "        chroma = librosa.feature.chroma_stft(audio, fs)\n",
    "        chroma_mean = np.mean(chroma, axis = 1)\n",
    "        spec_roll = librosa.feature.spectral_rolloff(audio, fs)\n",
    "        spec_roll_mean = np.mean(spec_roll, axis = 1)\n",
    "        spec_centr = librosa.feature.spectral_centroid(audio, fs)\n",
    "        spec_centr_mean = np.mean(spec_centr, axis = 1)\n",
    "        test_features[index, :] = np.concatenate((mfcc_mean, chroma_mean, spec_roll_mean, spec_centr_mean), axis = 0)\n",
    "    dict_test_features[c] = test_features\n",
    "    # print(dict_test_features[c].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MFCC PLOT\n",
    "\n",
    "for c in classes:\n",
    "    mfcc = mfcc_plot[c].transpose()\n",
    "    # Visualization\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(mfcc, origin='lower', aspect='auto')\n",
    "    plt.xlabel('Training samples')\n",
    "    plt.ylabel('MFCC coefficients')\n",
    "    plt.title('MFCC (coefficients 0 to 13) for class {}'.format(c))\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    mfcc_upper = mfcc[4:]\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(mfcc_upper, origin='lower', aspect='auto')\n",
    "    plt.title('MFCC (coefficients 4 to 13) for class {}'.format(c))\n",
    "    plt.xlabel('Training samples')\n",
    "    plt.ylabel('MFCC coefficients')\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0 = 'rock'\n",
    "class_1 = 'pop'\n",
    "class_2 = 'reggae'\n",
    "class_3 = 'disco'\n",
    "\n",
    "X_train_0 = dict_train_features[class_0]\n",
    "X_train_1 = dict_train_features[class_1]\n",
    "X_train_2 = dict_train_features[class_2]\n",
    "X_train_3 = dict_train_features[class_3]\n",
    "\n",
    "y_train_0 = np.zeros((X_train_0.shape[0],))\n",
    "y_train_1 = np.ones((X_train_1.shape[0],))\n",
    "y_train_2 = np.ones((X_train_2.shape[0],))*2\n",
    "y_train_3 = np.ones((X_train_3.shape[0],))*3\n",
    "\n",
    "#y_train = np.concatenate((y_train_class_0, y_train_class_1, y_train_class_1), axis=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_0 = dict_test_features[class_0]\n",
    "X_test_1 = dict_test_features[class_1]\n",
    "X_test_2 = dict_test_features[class_2]\n",
    "X_test_3 = dict_test_features[class_3]\n",
    "\n",
    "\n",
    "y_test_0 = np.zeros((X_test_0.shape[0],))\n",
    "y_test_1 = np.ones((X_test_1.shape[0],))\n",
    "y_test_2 = np.ones((X_test_2.shape[0],))*2\n",
    "y_test_3 = np.ones((X_test_1.shape[0],))*3\n",
    "\n",
    "y_test_mc = np.concatenate((y_test_0, y_test_1, y_test_2, y_test_3), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_max = np.max(np.concatenate((X_train_0, X_train_1, X_train_2, X_train_3), axis=0), axis=0)\n",
    "feat_min = np.min(np.concatenate((X_train_0, X_train_1, X_train_2, X_train_3), axis=0), axis=0)\n",
    "\n",
    "X_train_0_normalized = (X_train_0 - feat_min) / (feat_max - feat_min)\n",
    "X_train_1_normalized = (X_train_1 - feat_min) / (feat_max - feat_min)\n",
    "X_train_2_normalized = (X_train_2 - feat_min) / (feat_max - feat_min)\n",
    "X_train_3_normalized = (X_train_3 - feat_min) / (feat_max - feat_min)\n",
    "\n",
    "X_test_0_normalized = (X_test_0 - feat_min) / (feat_max - feat_min)\n",
    "X_test_1_normalized = (X_test_1 - feat_min) / (feat_max - feat_min)\n",
    "X_test_2_normalized = (X_test_2 - feat_min) / (feat_max - feat_min) # we concatenate to obtain max/min\n",
    "X_test_3_normalized = (X_test_3 - feat_min) / (feat_max - feat_min)\n",
    "\n",
    "X_test_mc_normalized = np.concatenate((X_test_0_normalized, X_test_1_normalized, X_test_2_normalized, X_test_3_normalized), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and train a model for each couple of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_parameters={\n",
    "    'C': 1,\n",
    "    'kernel': 'rbf',\n",
    "}\n",
    "\n",
    "clf_01 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "clf_02 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "clf_03 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "\n",
    "clf_12 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "clf_13 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n",
    "\n",
    "clf_23 = sklearn.svm.SVC(**SVM_parameters, probability=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_01.fit(np.concatenate((X_train_0_normalized, X_train_1_normalized), axis=0), \n",
    "           np.concatenate((y_train_0, y_train_1), axis=0))\n",
    "           \n",
    "clf_02.fit(np.concatenate((X_train_0_normalized, X_train_2_normalized), axis=0), \n",
    "           np.concatenate((y_train_0, y_train_2), axis=0))\n",
    "\n",
    "clf_03.fit(np.concatenate((X_train_0_normalized, X_train_3_normalized), axis=0), \n",
    "           np.concatenate((y_train_0, y_train_3), axis=0))\n",
    "\n",
    "\n",
    "clf_12.fit(np.concatenate((X_train_1_normalized, X_train_2_normalized), axis=0), \n",
    "           np.concatenate((y_train_1, y_train_2), axis=0)) \n",
    "\n",
    "clf_13.fit(np.concatenate((X_train_1_normalized, X_train_3_normalized), axis=0), \n",
    "           np.concatenate((y_train_1, y_train_3), axis=0)) \n",
    "\n",
    "\n",
    "clf_23.fit(np.concatenate((X_train_2_normalized, X_train_3_normalized), axis=0), \n",
    "           np.concatenate((y_train_2, y_train_3), axis=0)) \n",
    "#now we have a tree of model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we predict test on label. They're all binary classifiers. We have 3 prediction and we must extract the most present.\n",
    "y_test_predicted_01 = clf_01.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "y_test_predicted_02 = clf_02.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "y_test_predicted_03 = clf_03.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "\n",
    "y_test_predicted_12 = clf_12.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "y_test_predicted_13 = clf_13.predict(X_test_mc_normalized).reshape(-1, 1)\n",
    "\n",
    "y_test_predicted_23 = clf_23.predict(X_test_mc_normalized).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_mc = np.concatenate((y_test_predicted_01, y_test_predicted_02, y_test_predicted_03, y_test_predicted_12, y_test_predicted_13, y_test_predicted_23 ), axis=1)\n",
    "y_test_predicted_mc = np.array(y_test_predicted_mc, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_mv = np.zeros((y_test_predicted_mc.shape[0],))\n",
    "for i, e in enumerate(y_test_predicted_mc):\n",
    "    y_test_predicted_mv[i] = np.bincount(e).argmax() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we fill the matrix for each sample\n",
    "def compute_cm_multiclass(gt, predicted):\n",
    "    classes = np.unique(gt)\n",
    "    \n",
    "    CM = np.zeros((len(classes), len(classes)))\n",
    "    \n",
    "    for i in np.arange(len(classes)):\n",
    "        pred_class = predicted[gt==i]\n",
    "        \n",
    "        for j in np.arange(len(pred_class)): #the element in position\"label\" and the element in predicted class \n",
    "            CM[i, int(pred_class[j])] = CM[i, int(pred_class[j])] + 1 \n",
    "    print(CM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_cm_multiclass(y_test_mc, y_test_predicted_mv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
